{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Reconnaissance d'image](https://github.com/wikistat/Ateliers-Big-Data/2-MNIST) ([Cats-VS-Dogs](https://www.kaggle.com/c/dogs-vs-cats/data)) en <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 120px; display: inline\" alt=\"Python\"/></a> avec <a href=\"https://keras.io/\"><img src=\"https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png\" style=\"max-width: 100px; display: inline\" alt=\"Keras\"/></a>\n",
    "\n",
    "Les deux objectifs principaux de ce TP sont : \n",
    "\n",
    "1/ Utiliser des modèle de réseaux **Pré-entrainés** tel que `VGG16` ou `InceptionV3`.\n",
    "\n",
    "Apprendre un réseau convolutionel peut-être très couteux, même avec des technologies adaptées. Pour palier à ce problème il est possible d'utiliser des réseaux *Pré-entrainés*. Ces réseaux possèdent une structure particulière, établie après plusieurs itérations dans différents département de recherche (Microsoft (Resnet), Google (Inception V3), Facebook (ResNeXt). Ces réseaux ont ensuite été ajustés sur des banques d'images publiques tel que [ImageNet](http://www.image-net.org/). \n",
    "Ces modèles permettent ainsi d'exploiter la connaissance acquise sur un problème de classification général pour l’appliquer de nouveau à un problème particulier.\n",
    "\n",
    "2/ Comparer la performance des algorithmes de Keras sur CPU et GPU.\n",
    "\n",
    "La librairie Keras permet d'utiliser les performances d'une carte GPU pour améliorer le temps de calcul nécessaire à l'aprentissage et la prédiction d'un réseau de neurones. \n",
    "\n",
    "Ce TP est en grande partie inspiré du [blog](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) de François Chollet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Deep Learning Librairies\n",
    "import tensorflow as tf\n",
    "import keras.preprocessing.image as kpi\n",
    "import keras.layers as kl\n",
    "import keras.optimizers as ko\n",
    "import keras.backend as k\n",
    "import keras.models as km\n",
    "import keras.applications as ka\n",
    "\n",
    "# Visualisaiton des données\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# fix seed for reproducible results (only works on CPU, not GPU)\n",
    "seed = 42\n",
    "np.random.seed(seed=seed)\n",
    "tf.set_random_seed(seed=seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La commande suivante permet de verifier qu'une carte GPU est bien disponible sur la machine que vous utilisez. Si c'est le cas et i Keras a été installé dans la configuration GPU (ce qui est le cas dans l'environement virtuel GPU d'anaconda), deux options vont apparaitre, une CPU et une GPU. La configuration GPU sera alors automatiquement utilisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 18446327283331390089\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU\n"
     ]
    }
   ],
   "source": [
    "MODE = \"GPU\" if \"GPU\" in [k.device_type for k in device_lib.list_local_devices()] else \"CPU\"\n",
    "print(MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Présentation des données "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure des données et création d'un jeu d'apprentissage et de validation\n",
    "\n",
    "Les données originales peuvent être téléchargées sur la page [kaggle](https://www.kaggle.com/c/dogs-vs-cats/data).\n",
    "\n",
    "Le jeu de données d'apprentissage contient 25.000 images. Ce qui est beaucoup trop pour pouvoir être utilisé sur les machines des salles de TP. \n",
    "\n",
    "Nous avonons donc créer deux sous-échantillons d'apprentissage consitués de \n",
    "\n",
    "    1/ 100 images de chats et 100 images de chiens et un échantillon de validation consituée de 40 images de chats et 40 images de chien. [LIEN1 WIKISTAT]\n",
    "\n",
    "    2/ 1000 images de chats et 1000 images de chiens et un échantillon de validation consituée de 400 images de chats et 400 images de chien. [LIEN2 WIKISTAT]\n",
    "\n",
    "Télécharger ces données, dezipez les et placez les dans le dossier spécifier dans la variable `data_dir`\n",
    "\n",
    "\n",
    "Pour utiliser certaines foncitonalités de Keras, les données doivent être organisée selon une abrorescence précise. Les fichiers appartenant à une même classe doivent être dans un même dossier. \n",
    "\n",
    "```\n",
    "data_dir\n",
    "└───subsample/\n",
    "│   └───train/\n",
    "│   │   └───cats/\n",
    "│   │   │   │   cat.0.jpg\n",
    "│   │   │   │   cat.1.jpg\n",
    "│   │   │   │   ...\n",
    "│   │   └───dogs/\n",
    "│   │   │   │   dog.0.jpg\n",
    "│   │   │   │   dog.1.jpg\n",
    "│   │   │   │   ...\n",
    "│   └───test/\n",
    "│   │   └───cats/\n",
    "│   │   │   │   cat.1000.jpg\n",
    "│   │   │   │   cat.1000.jpg\n",
    "│   │   │   │   ...\n",
    "│   │   └───dogs/\n",
    "│   │   │   │   dog.1000.jpg\n",
    "│   │   │   │   dog.1000.jpg\n",
    "│   │   │   │   ...\n",
    "```\n",
    "\n",
    "Ainsi, si vous souhaitez télécharger les données originales et recréer des sous-échantillons de plus grandes tailles, vous devez respecter cette structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Spécifiez ici la direction du dossier dans lequel sont situées vos données, ainsi que la taille de votre echantillon d'apprentissage et de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = ''\n",
    "\n",
    "N_train = 200\n",
    "N_val = 80\n",
    "\n",
    "data_dir_sub = data_dir+'/subsample_%dN_train_%d_N_val' %(N_train, N_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration des données\n",
    "\n",
    "La fonction `load_img` permet de charger une image come une image PIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/subsample_200N_train_80_N_val/train/cats/cat.1.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-67f77a5dcd98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkpi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir_sub\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/train/cats/cat.1.jpg'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# this is a PIL image\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\Anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, target_size)\u001b[0m\n\u001b[0;32m    320\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[0;32m    321\u001b[0m                           'The use of `array_to_img` requires PIL.')\n\u001b[1;32m--> 322\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgrayscale\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'L'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2476\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2477\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2478\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/subsample_200N_train_80_N_val/train/cats/cat.1.jpg'"
     ]
    }
   ],
   "source": [
    "img = kpi.load_img(data_dir_sub+'/train/cats/cat.1.jpg')  # this is a PIL image\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `img_to_array` permet de générer un array numpy a partir d'une image PIL ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = kpi.img_to_array(img)  \n",
    "plt.imshow(x/255, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pré-traitement\n",
    "\n",
    "Les images du jeu de données sont de dimensions différentes : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_0 = kpi.img_to_array(kpi.load_img(data_dir_sub+\"/train/cats/cat.0.jpg\"))\n",
    "x_1 = kpi.img_to_array(kpi.load_img(data_dir_sub+\"/train/cats/cat.1.jpg\"))\n",
    "x_0.shape, x_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour palier à ce problème, nous pouvons utiliser la fonction `ImageDataGenerator`de `Keras`. \n",
    "\n",
    "Cette fonction permet également d'appliquer un certain nombre de traitements (transformation, normalisation) aléatoires sur les images de sorte que le modèle n'apprenne jamais deux fois la même image.\n",
    "\n",
    "Parmis les différents arguments de cette fonction nous trouvons entre autre:\n",
    "    * rotation_range : Un interval représentant les degrés possibles de rotation de l'image\n",
    "    * width_shift and height_shift : Un interval au sein duquel les données pourront être translatées horizontalement ou verticalement \n",
    "    * rescale :  Une valeur par lequelle les données seront multipliées\n",
    "    * shear_range :  Transvection\n",
    "    * zoom_range : Permet des zoom au sein d'une image\n",
    "    * horizontal_flip : Inverse aléatoirement des images selon l'axe horizontal\n",
    "    * fill_mode : La strategie adopté pour comblé les pixel manquant après une transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = kpi.ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons appliquer ces tranformations sur un exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_width = 150\n",
    "img_height = 150\n",
    "\n",
    "img = kpi.load_img(data_dir_sub+\"/train/cats/cat.1.jpg\")  # this is a PIL image\n",
    "x = kpi.img_to_array(img)  \n",
    "x_ = x.reshape((1,) + x.shape)\n",
    "\n",
    "if not(os.path.isdir(data_dir_sub+\"/preprocessing_example\")):\n",
    "    os.mkdir(data_dir_sub+\"/preprocessing_example\")\n",
    "\n",
    "    # the .flow() command below generates batches of randomly transformed images\n",
    "    # and saves the results to the `preview/` directory\n",
    "    i = 0\n",
    "    for batch in datagen.flow(x_, batch_size=1,save_to_dir=data_dir_sub+\"/preprocessing_example\", save_prefix='cat', save_format='jpeg'):\n",
    "        i += 1\n",
    "        if i > 7:\n",
    "            break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_list=[]\n",
    "for f in os.listdir(data_dir_sub+\"/preprocessing_example\"):\n",
    "    X_list.append(kpi.img_to_array(kpi.load_img(data_dir_sub+\"/preprocessing_example/\"+f)))\n",
    "\n",
    "\n",
    "fig=plt.figure(figsize=(16,8))\n",
    "fig.patch.set_alpha(0)\n",
    "ax = fig.add_subplot(3,3,1)\n",
    "ax.imshow(x/255, interpolation=\"nearest\")\n",
    "ax.set_title(\"Image original\")\n",
    "for i,xt in enumerate(X_list):\n",
    "    ax = fig.add_subplot(3,3,i+2)\n",
    "    ax.imshow(xt/255, interpolation=\"nearest\")\n",
    "    ax.set_title(\"Random transformation %d\" %(i+1))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/Users/bguillouet/Insa/DeepLearning/plot/cats_vs_dogs/cats_transformation.png\", dpi=100, bbox_to_anchor=\"tight\", facecolor=fig.get_facecolor())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification d'image à l'aide du Deep Learning\n",
    "\n",
    "Dans un premier temps, nous allons fixer le nombre d'epochs ainsi que la taille de notre batch afin que ces deux paramètres soit communs aux différentes méthodes que nous allons tester. \n",
    "Queques règles à suivre pour le choix de ces paramètres :\n",
    "\n",
    "* `epochs`: Commencer avec un nombre d'epochs relativement faible (2,3) afin de voir le temps de calcul nécessaire à votre machine, puis augmenter le en conséquence.\n",
    "* `batch_size`: La taille du batch correspond au nombre d'éléments qui seront étudié a chaque itération au cours d'une epochs. \n",
    "**Important** : Avec Keras, la taille du batch doit être un diviseur de la taille de l'échantillon. Sinon l'algorithme aura des comportement anormaux qui ne généreront pas forcément un message d'erreur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un réseaux convolutionnel\n",
    "\n",
    "Dans un premiers temps, on construit notre propre réseau de neurones convolutionnel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération des données\n",
    "\n",
    "On définit deux objets `ImageDataGenerator` :\n",
    "\n",
    "* `train_datagen`: pour l'apprentissage, où différentes transformations sont appliquées, comme précédement\n",
    "* `valid_datagen`: pour la validation, où l'on applique seulement une transformation *rescale* pour ne pas déformer les données.\n",
    "\n",
    "Il est également important de définir la taille des images dans laquelle nos images seront reformatées. Ici nous choisirons un taille d'image de 150x150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = kpi.ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "valid_datagen = kpi.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        data_dir_sub+\"/train/\",  # this is the target directory\n",
    "        target_size=(img_width, img_height),  \n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "        data_dir_sub+\"/validation/\",\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition du modèle\n",
    "\n",
    "Le modèle est consitué de 3 blocs de convolution consitutés chacun de:\n",
    "\n",
    "* Une couche de `Convolution2D`\n",
    "* Une couche d'`Activation` ReLU\n",
    "* Une couche `MaxPooling2D`\n",
    "\n",
    "Suivi de :\n",
    "* Une couche `Flatten`, permettant de convertir les features de 2 à 1 dimensions. \n",
    "* Une couche `Dense` (Fully connected layer)\n",
    "* Une couche d' `Activation` ReLU\n",
    "* Une couche `Dropout`\n",
    "* Une couche `Dense` de taille 1 suivi d'une `Activation` sigmoid permettant la classification binaire\n",
    "\n",
    "On utilise la fonction de perte `binary_crossentropy` pour apprendre notre modèle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "model_conv = km.Sequential()\n",
    "model_conv.add(kl.Conv2D(32, (3, 3), input_shape=(img_width, img_height, 3), data_format=\"channels_last\"))\n",
    "model_conv.add(kl.Activation('relu'))\n",
    "model_conv.add(kl.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_conv.add(kl.Conv2D(32, (3, 3)))\n",
    "model_conv.add(kl.Activation('relu'))\n",
    "model_conv.add(kl.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_conv.add(kl.Conv2D(64, (3, 3)))\n",
    "model_conv.add(kl.Activation('relu'))\n",
    "model_conv.add(kl.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_conv.add(kl.Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model_conv.add(kl.Dense(64))\n",
    "model_conv.add(kl.Activation('relu'))\n",
    "model_conv.add(kl.Dropout(0.5))\n",
    "model_conv.add(kl.Dense(1))\n",
    "model_conv.add(kl.Activation('sigmoid'))\n",
    "\n",
    "model_conv.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_conv.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "model_conv.fit_generator(train_generator, steps_per_epoch=N_train // batch_size, epochs=epochs,\n",
    "       validation_data=validation_generator,validation_steps=N_val // batch_size)\n",
    "te = time.time()\n",
    "t_learning_conv_simple_model = te-ts\n",
    "print(\"Learning TIme for %d epochs : %d seconds\"%(epochs,t_learning_conv_simple_model))\n",
    "model_conv.save(data_dir_sub+'/'+MODE+'_models_convolutional_network_%d_epochs_%d_batch_size.h5' %(epochs, batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "score_conv_val = model_conv.evaluate_generator(validation_generator, N_val //batch_size)\n",
    "score_conv_train = model_conv.evaluate_generator(train_generator, N_train// batch_size)\n",
    "te = time.time()\n",
    "t_prediction_conv_simple_model = te-ts\n",
    "print('Train accuracy:', score_conv_train[1])\n",
    "print('Test accuracy:', score_conv_val[1])\n",
    "print(\"Time Prediction: %.2f seconds\" %t_prediction_conv_simple_model )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metadata_conv = {\"score_val\" : score_conv_val, \"score_train\" : score_conv_train,\n",
    "                 \"t_learning\" : t_learning_conv_simple_model, \"t_prediction\" : t_prediction_conv_simple_model}\n",
    "pickle.dump(metadata_conv, open(data_dir_sub+'/'+MODE+'_metadata_convolutional_network_%d_epochs_%d_batch_size.pkl' %(epochs, batch_size), \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réseau pré-entrainé : VGG16\n",
    "\n",
    "Nous allons voir dans cette partie deux façon d'utiliser un modèle pré-entrainé:\n",
    "\n",
    "1/ Dans un premier temps on utilise le modèle pour extraire des features des images. Ces features sont le résultats des transformations des différents blocs de convolution sur nos images.  \n",
    "\n",
    "2/ Dans un second temps on utilisera le modèle comme une initialisation du modèle, qui est ensuite ré-entraîné plus finement (Fine Tuning) sur le dernier bloc de convolution pour traiter le nouveau problème de classification.\n",
    "\n",
    "\n",
    "### Illustration du réseau\n",
    "\n",
    "![](https://blog.keras.io/img/imgclf/vgg16_original.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Extraction de nouvelle Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Téléchargement des poids du modèle\n",
    "\n",
    "Si cest la première fois que vous appeler l'application `VGG16`, le lancement des poids commencera automatiquement et seront stocké dans votre home : `\"~/.keras/models\"`\n",
    "\n",
    "On utilise le modèle avec l'option `ìnclude_top` = False. C'est à dire que l'on ne télécharge pas le dernier bloc `Fully connected` classifier. \n",
    "\n",
    "La fonction `summary` permet de retrouver la structure décrite précédemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_VGG16_without_top = ka.VGG16(include_top=False, weights='imagenet')\n",
    "model_VGG16_without_top.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création des features\n",
    "\n",
    "On applique alors les 5 blocs du modèle VGG16 sur les images de nos échantillons d'apprentissage et de validation.\n",
    "\n",
    "Cette opération peut-être couteuse, c'est pourquoi on va sauver ces features dans des fichiers.\n",
    "Si ces fichiers existent, les poids seront téléchargés, sinon il seront créés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train_path = data_dir_sub+'/'+MODE+'_features_train_%d_epochs_%d_batch_size.npy' %(epochs, batch_size)\n",
    "features_validation_path = data_dir_sub+'/'+MODE+'_features_validation_%d_epochs_%d_batch_size.npy' %(epochs, batch_size)\n",
    "\n",
    "if os.path.isfile(features_train_path) and os.path.isfile(features_validation_path):\n",
    "    print(\"Load Features\")\n",
    "    features_train = np.load(open(features_train_path, \"rb\"))\n",
    "    features_validation = np.load(open(features_validation_path, \"rb\"))\n",
    "    \n",
    "else:\n",
    "    print(\"Generate Features\")\n",
    "    datagen = kpi.ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "\n",
    "    ts = time.time()\n",
    "    generator = datagen.flow_from_directory(\n",
    "            data_dir_sub+\"/train\",\n",
    "            target_size=(img_width, img_height),\n",
    "            batch_size=batch_size,\n",
    "            class_mode=None,  # this means our generator will only yield batches of data, no labels\n",
    "            shuffle=False)  # our data will be in order, so all first 1000 images will be cats, then 1000 dogs\n",
    "    # the predict_generator method returns the output of a model, given\n",
    "    # a generator that yields batches of numpy data\n",
    "    features_train = model_VGG16_without_top.predict_generator(generator, N_train // batch_size,  verbose = 1)\n",
    "    # save the output as a Numpy array\n",
    "    np.save(open(features_train_path, 'wb'), features_train)\n",
    "    te=time.time()\n",
    "    t_features_train = te-ts\n",
    "    ts = time.time()\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        data_dir_sub+\"/validation\",\n",
    "            target_size=(img_width, img_height),\n",
    "            batch_size=batch_size,\n",
    "            class_mode=None,\n",
    "            shuffle=False)\n",
    "    features_validation = model_VGG16_without_top.predict_generator(generator, N_val //batch_size,  verbose = 1)\n",
    "\n",
    "\n",
    "    np.save(open(features_validation_path, 'wb'), features_validation)\n",
    "    te=time.time()\n",
    "    t_features_validation = te-ts\n",
    "    \n",
    "    pickle.dump([t_features_train, t_features_validation], open(data_dir_sub+'/'+MODE+'_time_features_train_%d_epochs_%d_batch_size.npy' %(epochs,batch_size), \"wb\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construction d'un réseaux de neurone classique.\n",
    "\n",
    "On construit un réseaux de neurones \"classique\", identique à la seconde partie du réseau précédent. \n",
    "\n",
    "*Attention* : La première couche de ce réseaux (`Flatten`) doit être configuré pour prendre en compte des données dans la dimension des features générées précédemment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e5f9571d4ee0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeatures_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'features_train' is not defined"
     ]
    }
   ],
   "source": [
    "features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_VGG_fcm = km.Sequential()\n",
    "model_VGG_fcm.add(kl.Flatten(input_shape=features_train.shape[1:]))\n",
    "model_VGG_fcm.add(kl.Dense(64, activation='relu'))\n",
    "model_VGG_fcm.add(kl.Dropout(0.5))\n",
    "model_VGG_fcm.add(kl.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_VGG_fcm.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_VGG_fcm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts=time.time()\n",
    "\n",
    "# On créer des vecteurs labels\n",
    "\n",
    "train_labels = np.array([0] * int((N_train/2)) + [1] * int((N_train/2)))\n",
    "validation_labels = np.array([0] * int((N_val/2)) + [1] * int((N_val/2)))\n",
    "\n",
    "model_VGG_fcm.fit(features_train, train_labels,\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(features_validation, validation_labels))\n",
    "te = time.time()\n",
    "t_learning_VGG_fcm = te-ts\n",
    "model_VGG_fcm.save(data_dir_sub+'/'+MODE+'_models_VGG_fully_connected_model_%d_epochs_%d_batch_size.h5' %(epochs, batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons également sauver les poids de ce modèle afin de les réusiliser dans la prochaine partie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_VGG_fcm.save_weights(data_dir_sub+'/'+MODE+'_weights_model_VGG_fully_connected_model_%d_epochs_%d_batch_size.h5' %(epochs, batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "score_VGG_fcm_val = model_VGG_fcm.evaluate(features_validation, validation_labels)\n",
    "score_VGG_fcm_train = model_VGG_fcm.evaluate(features_train, train_labels)\n",
    "te = time.time()\n",
    "t_prediction_VGG_fcm = te-ts\n",
    "print('Train accuracy:', score_VGG_fcm_val[1])\n",
    "print('Test accuracy:', score_VGG_fcm_train[1])\n",
    "print(\"Time Prediction: %.2f seconds\" %t_prediction_VGG_fcm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metadata_VGG_fcm = {\"score_val\" : score_VGG_fcm_val, \"score_train\" : score_VGG_fcm_train, \n",
    "                    \"t_learning\" : t_learning_VGG_fcm, \"t_prediction\" : t_prediction_VGG_fcm}\n",
    "pickle.dump(metadata_VGG_fcm, open(data_dir_sub+'/'+MODE+'_metadata_VGG_fully_connected_model_%d_epochs_%d_batch_size.pkl' %(epochs, batch_size), \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning du modèle VGG16\n",
    "\n",
    "Dans la partie précédente, nous avons configurer un bloc de réseaux de neurones, à même de prendre en entrée les features issues des transformation des 5 premiers blocs de convolution du modèle VGG16. \n",
    "\n",
    "Dans cette partie, nous allons 'brancher' ce bloc directement sur les cinq premiers blocs du modèle VGG16 pour pouvoir affiner le modèle en itérant a la fois sur les blocs de convolution mais également sur notre bloc de réseau de neurone.\n",
    "\n",
    "![](https://blog.keras.io/img/imgclf/vgg16_modified.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description du modèle\n",
    "\n",
    "On télécharge dans un premier temps le modèle VGG16, comme précédement. \n",
    "Cependant, le modèle va cette fois être \"entrainé\" directement, et ne va pas servir qu'a générer des features, il faut donc préciser en paramètre la taille des images que l'on va lui donner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the VGG16 network\n",
    "model_VGG16_without_top = ka.VGG16(include_top=False, weights='imagenet', input_shape=(150,150,3))\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On ajoute au modèle VGG, notre bloc de réseaux de neuronne construit précédemment. \n",
    "Pour cela, on construit le bloc comme précédemment, puis on y ajoute les poids issus de l'apprentissage également réasilsé précédemment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = km.Sequential()\n",
    "top_model.add(kl.Flatten(input_shape=model_VGG16_without_top.output_shape[1:]))\n",
    "top_model.add(kl.Dense(64, activation='relu'))\n",
    "top_model.add(kl.Dropout(0.5))\n",
    "top_model.add(kl.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "\n",
    "top_model.load_weights(data_dir_sub+'/'+MODE+'_weights_model_VGG_fully_connected_model_%d_epochs_%d_batch_size.h5' %(epochs, batch_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis on concatane les deux parties du modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add the model on top of the convolutional base\n",
    "model_VGG_LastConv_fcm = km.Model(inputs=model_VGG16_without_top.input, outputs=top_model(model_VGG16_without_top.output))\n",
    "\n",
    "model_VGG_LastConv_fcm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On 'Gèle' les 4 premiers blocs de convolution\n",
    "\n",
    "En pratique, et pour pouvoir effectuer ces calculs dans un temps raisonable, nous allons \"fine-tuner\" seulement le dernier bloc de convolution du modèle, le bloc 5 (couches 16 à 19 dans le summary du modèle précédent) ainsi que le bloc de réseau de neurones que nous avons ajoutés. \n",
    "\n",
    "Pour cela on va \"geler\" (Freeze) les 15 premières couches du modèle pour que leur paramètre ne soit pas optimiser pendant la phase d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the first 25 layers (up to the last conv bloc)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model_VGG_LastConv_fcm.layers[:15]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "    \n",
    "    \n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model_VGG_LastConv_fcm.compile(loss='binary_crossentropy',\n",
    "              optimizer=ko.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare data augmentation configuration\n",
    "train_datagen = kpi.ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = kpi.ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir_sub+\"/train/\",\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    data_dir_sub+\"/validation/\",\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fine-tune the model\n",
    "ts = time.time()\n",
    "model_VGG_LastConv_fcm.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=N_train // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=N_val // batch_size)\n",
    "te = time.time()\n",
    "t_learning_VGG_LastConv_fcm = te-ts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "score_VGG_LastConv_fcm_val = model_VGG_LastConv_fcm.evaluate_generator(validation_generator, N_val // batch_size)\n",
    "score_VGG_LastConv_fcm_train = model_VGG_LastConv_fcm.evaluate_generator(train_generator, N_train // batch_size)\n",
    "\n",
    "te = time.time()\n",
    "t_prediction_VGG_LastConv_fcm = te-ts\n",
    "print('Train accuracy:', score_VGG_LastConv_fcm_val[1])\n",
    "print('Test accuracy:', score_VGG_LastConv_fcm_train[1])\n",
    "print(\"Time Prediction: %.2f seconds\" %t_prediction_VGG_LastConv_fcm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metadata_VGG_LastConv_fcm = {\"score_val\" : score_VGG_LastConv_fcm_val, \"score_train\" : score_VGG_LastConv_fcm_train, \n",
    "                             \"t_learning\" : t_learning_VGG_LastConv_fcm, \"t_prediction\" : t_prediction_VGG_LastConv_fcm}\n",
    "pickle.dump(metadata_VGG_LastConv_fcm, open(data_dir_sub+'/'+MODE+'_metadata_VGG_LastConv_fully_connected_model_%d_epochs_%d_batch_size.pkl' %(epochs, batch_size), \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Autres modèles\n",
    "\n",
    "Keras possède un certain nombre d'autre modèle pré-entrainé :\n",
    "\n",
    "* Xception\n",
    "* VGG16\n",
    "* VGG19\n",
    "* ResNet50\n",
    "* InceptionV3\n",
    "* InceptionResNetV2\n",
    "* MobileNet\n",
    "\n",
    "Certain possède une structure bien plus complexe, notamment `InceptionV3`. Vous pouvez très facilement remplacer la fonction `ka.VGG16` par une autre fonction (ex : `ka.InceptionV3`) pour tester la performance des ces différents modèles et leur complexité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "865px",
    "left": "0px",
    "right": "1587.01px",
    "top": "106px",
    "width": "213px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
